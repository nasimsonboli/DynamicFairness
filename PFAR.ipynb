{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAR rerank\n",
    "# Based on W. Liu, R. Burke, Personalizing Fairness-aware Re-ranking\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from librec_auto import read_config_file\n",
    "from pathlib import Path\n",
    "\n",
    "class FarHelper:\n",
    "    item_feature_df = None\n",
    "    protected = None\n",
    "    lam = 0\n",
    "    max_length = 10\n",
    "    binary = True\n",
    "    protected_set = {}\n",
    "\n",
    "    def is_protected(self, itemid):\n",
    "        return itemid in self.protected_set\n",
    "\n",
    "    def num_prot(self, items):\n",
    "        num_prot = [self.is_protected(itemid) for itemid in items].count(True)\n",
    "        return num_prot\n",
    "\n",
    "def entropy_(labels, base=None):\n",
    "    from math import log, e\n",
    "    \"\"\" Computes entropy of label distribution. \"\"\"\n",
    "    n_labels = len(labels)\n",
    "    if n_labels <= 1:\n",
    "        return 0\n",
    "\n",
    "    value, counts = np.unique(labels, return_counts=True)\n",
    "    probs = counts / n_labels\n",
    "    n_classes = np.count_nonzero(probs)\n",
    "\n",
    "    if n_classes <= 1:\n",
    "        return 0\n",
    "    ent = 0.\n",
    "    # Compute entropy\n",
    "    base = e if base is None else base\n",
    "    for i in probs:\n",
    "        ent -= i * log(i, base)\n",
    "\n",
    "    return ent\n",
    "\n",
    "def get_user_tolerance(user_profile, item_features, helper):\n",
    "    # look through the items that the user has rated before\n",
    "    # look through the item features and save a list of all these feature names in a file\n",
    "    # call entropy_() function and return the entropy result\n",
    "    user_items = user_profile['itemid'].tolist()\n",
    "    if len(user_items) == 0:\n",
    "        return 0\n",
    "    return entropy_(item_features.loc[item_features.index.isin(user_items), 'feature'].tolist())\n",
    "\n",
    "\n",
    "# Caches the protected items for quicker lookup\n",
    "def get_protected_set(item_features, helper):\n",
    "    return set((item_features[(item_features['feature']==helper.protected) & (item_features['value']==1)].index).tolist())\n",
    "\n",
    "#def is_protected(itemid):\n",
    "#    item_entry = item_feature_df.loc[itemid]\n",
    "#    prot_item = item_entry[(item_entry['feature']==protected) & (item_entry['value']==1)]\n",
    "#    return type(prot_item) == numpy.int64\n",
    "\n",
    "# def num_prot(items):\n",
    "#     num_prot = [is_protected(itemid) for itemid in items].count(True)\n",
    "#     return num_prot\n",
    "\n",
    "def score_prot(user_profile, helper):\n",
    "    user_items = user_profile['itemid'].tolist()\n",
    "    if len(user_items)==0:\n",
    "        return 0\n",
    "    return helper.num_prot(user_items) / len(user_items)\n",
    "\n",
    "def rescore_binary(item, original_score, items_so_far, score_profile, user_tol, helper):\n",
    "    answer = original_score\n",
    "    div_term = 0\n",
    "\n",
    "    # If there are both kind of items in the list, no re-ranking happens\n",
    "    count_prot = helper.num_prot(items_so_far)\n",
    "    if helper.is_protected(item):\n",
    "        if count_prot==0:\n",
    "            div_term = score_profile\n",
    "    else:\n",
    "        if count_prot == len(items_so_far):\n",
    "            div_term = 1 - score_profile\n",
    "\n",
    "    div_term *= helper.lam\n",
    "    div_term *= user_tol\n",
    "    answer += div_term\n",
    "    return answer\n",
    "\n",
    "# Not in the original paper, but treats the P(\\\\bar{s)|d) as real-valued\n",
    "# See Abdollahpouri, Burke, and Mobasher. Managing popularity bias in recommender systems with personalized re-ranking. 2019\n",
    "def rescore_prop(item, original_score, items_so_far, score_profile, user_tol, helper):\n",
    "    answer = original_score\n",
    "    div_term = 0\n",
    "\n",
    "    count_prot = helper.num_prot(items_so_far)\n",
    "    count_items = len(items_so_far)\n",
    "    if count_items == 0:\n",
    "        div_term = score_profile\n",
    "    else:\n",
    "        if helper.is_protected(item):\n",
    "            div_term = score_profile\n",
    "            div_term *= 1 - count_prot / count_items\n",
    "        else:\n",
    "            div_term = (1 - score_profile)\n",
    "            div_term *= count_prot / count_items\n",
    "\n",
    "    div_term *= helper.lam\n",
    "    answer += div_term\n",
    "    div_term *= user_tol\n",
    "    return answer\n",
    "\n",
    "\n",
    "def pick_best(user_recs, user_profile, items_so_far, item_features, helper):\n",
    "    best_item = None\n",
    "    best_score = -1\n",
    "    score_profile = score_prot(user_profile, helper)\n",
    "    user_tol = get_user_tolerance(user_profile, item_features, helper)\n",
    "\n",
    "    for _, _, item, score in user_recs.itertuples():\n",
    "        if helper.binary:\n",
    "            new_score = rescore_binary(item, score, items_so_far, score_profile, user_tol, helper)\n",
    "        else:\n",
    "            new_score = rescore_prop(item, score, items_so_far, score_profile, user_tol, helper)\n",
    "        if new_score > best_score:\n",
    "            best_item = item\n",
    "            best_score = new_score\n",
    "\n",
    "    return (best_item, best_score)\n",
    "\n",
    "def rerank(userid, user_recs_df, user_profile, item_features, helper):\n",
    "    output_data = []\n",
    "    items_so_far = []\n",
    "\n",
    "    for i in range(0, helper.max_length):\n",
    "\n",
    "        item, score = pick_best(user_recs_df, user_profile, items_so_far, item_features, helper)\n",
    "\n",
    "        items_so_far.append(item)\n",
    "        output_data.append((userid, item, score))\n",
    "        new_user_recs = user_recs_df[user_recs_df['itemid']!=item]\n",
    "        user_recs_df = new_user_recs\n",
    "\n",
    "    return pd.DataFrame(output_data, columns=['userid', 'itemid', 'score'])\n",
    "\n",
    "def execute(recoms_df, train_df, item_features, helper):\n",
    "    result = []\n",
    "\n",
    "    for userid in list(set(recoms_df['userid'])):\n",
    "#        print('list reranked for user #',userid)\n",
    "        result.append(rerank(userid, recoms_df[recoms_df['userid']==userid].copy(),\n",
    "                             train_df[train_df['userid']==userid], item_features,\n",
    "                             helper))\n",
    "\n",
    "    rr_df = pd.concat(result)\n",
    "    return rr_df\n",
    "\n",
    "\n",
    "# def read_args():\n",
    "#     \"\"\"\n",
    "#     Parse command line arguments.\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     parser = argparse.ArgumentParser(description='Generic re-ranking script')\n",
    "#     parser.add_argument('conf', help='Name of configuration file')\n",
    "#     parser.add_argument('target', help='Experiment target')\n",
    "#     parser.add_argument('original', help='Path to original results directory')\n",
    "#     parser.add_argument('result', help='Path to destination results directory')\n",
    "#     parser.add_argument('--max_len', help='The maximum number of items to return in each list', default=10)\n",
    "#     parser.add_argument('--lambda', help='The weight for re-ranking.')\n",
    "#     parser.add_argument('--binary', help='Whether P(\\\\bar{s)|d) is binary or real-valued', default=True)\n",
    "\n",
    "#     input_args = parser.parse_args()\n",
    "#     return vars(input_args)\n",
    "\n",
    "\n",
    "# RESULT_FILE_PATTERN = 'out-\\d+.txt'\n",
    "# INPUT_FILE_PATTERN = 'cv_\\d+'\n",
    "\n",
    "# def enumerate_results(result_path):\n",
    "#     files = os.listdir(result_path)\n",
    "#     pat = re.compile(RESULT_FILE_PATTERN)\n",
    "#     return [file for file in files if pat.match(file)]\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# #     args = read_args()\n",
    "#     #print(args)\n",
    "# #     config = read_config_file(args['conf'], args['target'])\n",
    "# #     result_files = enumerate_results(args['original'])\n",
    "\n",
    "# #     split_path = config.get_files().get_split_path()\n",
    "#     # split_names = os.listdir(split_path)\n",
    "\n",
    "# #     data_dir = config.get_prop_dict()['dfs.data.dir']\n",
    "# #     item_feature_file = config.get_prop_dict()['data.itemfeature.path']\n",
    "# #     protected = config.get_prop_dict()['data.protected.feature']\n",
    "\n",
    "# #     item_feature_path = Path(data_dir) / item_feature_file\n",
    "\n",
    "#     item_feature_df = None\n",
    "#     item_feature_df = pd.read_csv(item_feature_path, names=['itemid', 'feature', 'value'])\n",
    "#     item_feature_df.set_index('itemid', inplace=True)\n",
    "\n",
    "# #     if not item_feature_path.exists():\n",
    "# #         print(\"Cannot locate item features. Path: \" + item_feature_path)\n",
    "# #         exit(-1)\n",
    "# #     else:\n",
    "# #         item_feature_df = pd.read_csv(item_feature_path, names=['itemid', 'feature', 'value'])\n",
    "# #         item_feature_df.set_index('itemid', inplace=True)\n",
    "\n",
    "#     helper = FarHelper()\n",
    "#     helper.protected_set = get_protected_set(item_feature_df, helper)\n",
    "#     helper.lam = float(args['lambda'])\n",
    "#     helper.max_length = int(args['max_len'])\n",
    "#     helper.binary = args['binary']=='True'\n",
    "#     helper.protected = protected\n",
    "    \n",
    "#     # training set\n",
    "#     tr_df = pd.read_csv(tr_file_path, names=['userid', 'itemid', 'score'], sep='\\t')\n",
    "#     # recoms\n",
    "#     recoms_df = pd.read_csv(input_file_path, names=['userid', 'itemid', 'score'])\n",
    "\n",
    "#     # generate the reranked list\n",
    "#     reranked_df = execute(recoms_df, tr_df, item_feature_df, helper)\n",
    "#     reranked_df.to_csv('./recs/pfar_rr_lam%s.csv'%lam, header=None, index=False)\n",
    "    \n",
    "    \n",
    "#     for file_name in result_files:\n",
    "\n",
    "        # reading the result\n",
    "#         input_file_path = Path(args['original'] + '/' + file_name)\n",
    "\n",
    "        # reading the training set\n",
    "#         cv_path = str(split_path) + '/cv_' + re.findall('\\d+', file_name)[0] + '/train.txt'\n",
    "#         tr_file_path = Path(cv_path)\n",
    "\n",
    "#         tr_df = None\n",
    "#         if tr_file_path.exists():\n",
    "#             tr_df = pd.read_csv(tr_file_path, names=['userid', 'itemid', 'score'], sep='\\t')\n",
    "#         else:\n",
    "#             print('Cannot locate training data: ' + tr_file_path)\n",
    "#             exit(-1)\n",
    "\n",
    "#         if input_file_path.exists():\n",
    "#             recoms_df = pd.read_csv(input_file_path, names=['userid', 'itemid', 'score'])\n",
    "\n",
    "#             reranked_df = execute(recoms_df, tr_df, item_feature_df, helper)\n",
    "\n",
    "#             output_file_path = Path(args['result'] + '/' + file_name)\n",
    "#             print('Reranking for ', output_file_path)\n",
    "#             reranked_df.to_csv(output_file_path, header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "item_feature_df = None\n",
    "item_feature_df = pd.read_csv('./data', names=['itemid', 'feature', 'value'])\n",
    "item_feature_df.set_index('itemid', inplace=True)\n",
    "\n",
    "\n",
    "helper = FarHelper()\n",
    "helper.protected_set = get_protected_set(item_feature_df, helper)\n",
    "helper.lam = float(args['lambda'])\n",
    "helper.max_length = int(args['max_len'])\n",
    "helper.binary = args['binary']=='True'\n",
    "helper.protected = protected\n",
    "\n",
    "# training set\n",
    "tr_df = pd.read_csv(tr_file_path, names=['userid', 'itemid', 'score'], sep='\\t')\n",
    "# recoms\n",
    "recoms_df = pd.read_csv(input_file_path, names=['userid', 'itemid', 'score'])\n",
    "\n",
    "# generate the reranked list\n",
    "reranked_df = execute(recoms_df, tr_df, item_feature_df, helper)\n",
    "reranked_df.to_csv('./recs/pfar_rr_lam%s.csv'%lam, header=None, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
